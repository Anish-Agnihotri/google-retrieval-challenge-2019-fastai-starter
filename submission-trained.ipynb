{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install faiss-gpu cudatoolkit=9.0 -c pytorch-y \n",
    "#Fast nearest neighbors will help us a lot :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from third_party.testdataset import configdataset\n",
    "from third_party.evaluate import compute_map_and_print\n",
    "\n",
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "import pandas as pd\n",
    "from arch import RingGeMNet, GeMNet\n",
    "from losses import RingLoss\n",
    "from utils import extract_vectors_batched\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/mnt/fry2/users/datasets/landmarkscvprw18/retr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/mnt/fry2/users/datasets/landmarkscvprw18/retr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/mnt/fry2/users/datasets/landmarkscvprw18/retr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/mnt/fry2/users/datasets/landmarkscvprw18/retr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/mnt/fry2/users/datasets/landmarkscvprw18/retr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Image\n",
       "0  /mnt/fry2/users/datasets/landmarkscvprw18/retr...\n",
       "1  /mnt/fry2/users/datasets/landmarkscvprw18/retr...\n",
       "2  /mnt/fry2/users/datasets/landmarkscvprw18/retr...\n",
       "3  /mnt/fry2/users/datasets/landmarkscvprw18/retr...\n",
       "4  /mnt/fry2/users/datasets/landmarkscvprw18/retr..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I suppose, that competition dataset is downloaded to COMP_DATA_DIR\n",
    "#and the list of images are in index_image_list_fullpath.txt\n",
    "COMP_DATA_DIR = '/mnt/fry2/users/datasets/landmarkscvprw18/retrieval'\n",
    "df = pd.read_csv(os.path.join(COMP_DATA_DIR, 'index_image_list_fullpath.txt'),\n",
    "                  usecols=[0], names=['Image'])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageListAbsPath(ImageList):\n",
    "    def open(self, fn:PathOrStr)->Image:\n",
    "        return open_image(fn.replace('./',''))\n",
    "tfms = get_transforms(do_flip=False)\n",
    "tfms = (tfms[1],tfms[1]) \n",
    "DO_FULL_SIZE = False \n",
    "# Extracting features from full-size images would take 15 hours. You probably want to do this \n",
    "#for real submission, not for sample. So lets do it on 256x256 images\n",
    "if DO_FULL_SIZE:\n",
    "    BS=1\n",
    "    NUM_WORKERS=8\n",
    "    data = (ImageListAbsPath.from_df(df,path='', cols=['Image'])\n",
    "            .split_none()\n",
    "            .label_const()\n",
    "            .transform(tfms, resize_method=ResizeMethod.NO)\n",
    "            .databunch(bs=BS, num_workers=NUM_WORKERS)\n",
    "            .normalize(imagenet_stats)\n",
    "           ) \n",
    "    data.train_dl.dl.batch_sampler.sampler = torch.utils.data.SequentialSampler(data.train_ds)\n",
    "    data.train_dl.dl.batch_sampler.drop_last = False\n",
    "if not DO_FULL_SIZE:\n",
    "    BS=64\n",
    "    NUM_WORKERS=8\n",
    "    data = (ImageListAbsPath.from_df(df,path='', cols=['Image'])\n",
    "            .split_none()\n",
    "            .label_const()\n",
    "            .transform(tfms, resize_method=ResizeMethod.SQUISH, size=192)\n",
    "            .databunch(bs=BS, num_workers=NUM_WORKERS)\n",
    "            .normalize(imagenet_stats)\n",
    "           ) \n",
    "    data.train_dl.dl.batch_sampler.sampler = torch.utils.data.SequentialSampler(data.train_ds)\n",
    "    data.train_dl.dl.batch_sampler.drop_last = False\n",
    "    #index_features = extract_vectors_batched(data,InferenceNet, BS)\n",
    "#torch.save(index_features, 'densenet121_pretrained_256px_index_feats.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home.dokt/mishkdmy/anaconda3/envs/fastai1/lib/python3.7/site-packages/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(m.weight.data)\n"
     ]
    }
   ],
   "source": [
    "#Now we will load training network and transfer its weights to the inference net\n",
    "TRAIN_CLASSES=780 #that is hardcoded from number of classes in training dataset\n",
    "learn = Learner(data, RingGeMNet(models.densenet121(pretrained=True), TRAIN_CLASSES),\n",
    "                   metrics=[accuracy],\n",
    "                   loss_func=nn.CrossEntropyLoss())\n",
    "\n",
    "InferenceNet =  GeMNet(models.densenet121())\n",
    "InferenceNet.cnn.load_state_dict(learn.model.cnn.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will take 30 min on Titan X\n",
    "if not DO_FULL_SIZE:\n",
    "    qdf = pd.read_csv(os.path.join(COMP_DATA_DIR, 'test_image_list_fullpath.txt'),\n",
    "                  usecols=[0], names=['Image'])\n",
    "    qdf.head()\n",
    "    BS=64\n",
    "    NUM_WORKERS=6\n",
    "    qdata = (ImageListAbsPath.from_df(qdf,path='', cols=['Image'])\n",
    "            .split_none()\n",
    "            .label_const()\n",
    "            .transform(tfms, resize_method=ResizeMethod.SQUISH, size=256)\n",
    "            .databunch(bs=BS, num_workers=NUM_WORKERS)\n",
    "            .normalize(imagenet_stats)\n",
    "           ) \n",
    "    qdata.train_dl.dl.batch_sampler.sampler = torch.utils.data.SequentialSampler(qdata.train_ds)\n",
    "    qdata.train_dl.dl.batch_sampler.drop_last = False\n",
    "    #query_features = extract_vectors_batched(qdata,InferenceNet, BS)\n",
    "#torch.save(query_features, 'densenet121_pretrained_256px_query_feats.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query_features = torch.load('densenet121_pretrained_256px_query_feats.pth').numpy()\n",
    "index_features = np.zeros((len(data.train_ds),1024)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets do the nearest neighbor search and create the submission\n",
    "import faiss\n",
    "\n",
    "query_fnames = [x.split('/')[-1].replace('.jpg','') for x in qdf.Image.tolist()]\n",
    "index_fnames = [x.split('/')[-1].replace('.jpg','') for x in df.Image.tolist()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='3631' class='' max='3631', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [3631/3631 00:56<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_submission.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='116166' class='' max='116166', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [116166/116166 00:01<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from fastprogress import master_bar, progress_bar\n",
    "\n",
    "def get_idxs_and_dists(query_features, index_features, BS = 32):\n",
    "    flat_config = faiss.GpuIndexFlatConfig()\n",
    "    flat_config.device = 0\n",
    "    res = faiss.StandardGpuResources()\n",
    "    co = faiss.GpuClonerOptions()\n",
    "    FEAT_DIM = index_features.shape[1]\n",
    "    cpu_index = faiss.IndexFlatL2(FEAT_DIM)\n",
    "    cpu_index.add(index_features)\n",
    "    index = faiss.index_cpu_to_gpu(res, 0, cpu_index, co)\n",
    "    out_dists = np.zeros((len(query_features), 100), dtype=np.float32)\n",
    "    out_idxs = np.zeros((len(query_features), 100), dtype=np.int32)\n",
    "    NUM_QUERY = len (query_features)\n",
    "    for ind in progress_bar(range(0, len(query_features), BS)):\n",
    "        fin = ind+BS\n",
    "        if fin > NUM_QUERY:\n",
    "            fin = NUM_QUERY\n",
    "        q_descs = query_features[ind:fin]\n",
    "        D, I = index.search(q_descs, 100)\n",
    "        out_dists[ind:fin] = D\n",
    "        out_idxs[ind:fin] = I\n",
    "    return out_idxs, out_dists\n",
    "\n",
    "def create_submission_from_features(query_features,\n",
    "                                    index_features,\n",
    "                                    fname,\n",
    "                                    query_fnames,\n",
    "                                    index_fnames):\n",
    "    out_idxs, out_dists = get_idxs_and_dists(query_features, index_features, BS = 32)\n",
    "    print (f'Writing {fname}')\n",
    "    with open(fname, 'w') as f:\n",
    "        f.write('id,images\\n')\n",
    "        for i in progress_bar(range(len(query_fnames))):\n",
    "            ids = [index_fnames[x] for x in out_idxs[i]]\n",
    "            f.write(query_fnames[i] + ',' + ' '.join(ids)+'\\n')\n",
    "    print('Done!')\n",
    "    return\n",
    "create_submission_from_features(query_features, index_features, 'test_submission.csv',\n",
    "                                   query_fnames, index_fnames)\n",
    "        \n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
